{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Imports for Data Handling ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "# --- Scikit-learn Imports for ML ---\n",
    "\n",
    "# 1. For splitting your data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 2. For converting text data into a matrix of numerical features (TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 3. The Naive Bayes classifier model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 4. For evaluating your model's performance\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"All necessary libraries are imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your dataset\n",
    "filepath = 'SMSSpamCollection'\n",
    "\n",
    "try:\n",
    "    # Load the dataset using pandas\n",
    "    # The SMSSpamCollection dataset is a TSV (tab-separated-value) file\n",
    "    # It also doesn't have a header row, so we set header=None\n",
    "    # We provide our own column names: 'label' and 'message'\n",
    "    df = pd.read_csv(filepath, sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "    # Display the first 5 rows to make sure it loaded correctly\n",
    "    print(\"--- Dataset Head ---\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\") # Adds a blank line for better readability\n",
    "\n",
    "    # Count the number of 'spam' and 'ham' messages\n",
    "    print(\"--- Message Counts (Ham vs. Spam) ---\")\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{filepath}' was not found.\")\n",
    "    print(\"Please make sure the file is in the same directory as your notebook, or update the 'filepath' variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your new dataset\n",
    "filepath_emails = 'emails.csv'\n",
    "\n",
    "try:\n",
    "    # Load the CSV dataset using pandas\n",
    "    # This file (unlike the last one) appears to have a header row\n",
    "    df_emails = pd.read_csv(filepath_emails)\n",
    "\n",
    "    # Display the first 5 rows to make sure it loaded correctly\n",
    "    print(\"--- Dataset Head (emails.csv) ---\")\n",
    "    print(df_emails.head())\n",
    "    print(\"\\n\") # Adds a blank line\n",
    "\n",
    "    # Count the number of 'spam' (1) and 'ham' (0) messages\n",
    "    print(\"--- Message Counts (0 = Ham, 1 = Spam) ---\")\n",
    "    # We can use value_counts() on the 'spam' column\n",
    "    print(df_emails['spam'].value_counts())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{filepath_emails}' was not found.\")\n",
    "    print(\"Please make sure the file is in the same directory as your notebook, or update the 'filepath_emails' variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your spambase dataset\n",
    "# The original file is often named 'spambase.data', but use '.csv' if you've saved it that way\n",
    "filepath_spambase = 'spambase.data' \n",
    "\n",
    "try:\n",
    "    # Load the dataset using pandas\n",
    "    # We set header=None because the file doesn't have a title row\n",
    "    df_spambase = pd.read_csv(filepath_spambase, header=None)\n",
    "\n",
    "    # Display the first 5 rows to make sure it loaded correctly\n",
    "    # You will see columns indexed from 0 to 57\n",
    "    print(\"--- Dataset Head (spambase.data) ---\")\n",
    "    print(df_spambase.head())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # For this dataset, the label is in the last column (index 57)\n",
    "    # 1 = spam, 0 = not spam\n",
    "    print(\"--- Message Counts (0 = Not Spam, 1 = Spam) ---\")\n",
    "    print(df_spambase[57].value_counts())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{filepath_spambase}' was not found.\")\n",
    "    print(\"Please make sure the file is in the same directory as your notebook, or update the 'filepath_spambase' variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51294e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your new CSV dataset\n",
    "filepath_better30 = 'BETTER30.csv'\n",
    "\n",
    "try:\n",
    "    # Load the CSV file using pandas\n",
    "    df_better30 = pd.read_csv(filepath_better30)\n",
    "\n",
    "    # Display the first 5 rows to make sure it loaded correctly\n",
    "    print(\"--- Dataset Head (BETTER30.csv) ---\")\n",
    "    print(df_better30.head())\n",
    "    print(\"\\n\") # Adds a blank line\n",
    "\n",
    "    # Count the number of messages in each category\n",
    "    print(\"--- Message Counts (by LABEL) ---\")\n",
    "    print(df_better30['LABEL'].value_counts())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{filepath_better30}' was not found.\")\n",
    "    print(\"Please make sure the file is in the same directory as your notebook, or update the 'filepath_better30' variable.\")\n",
    "except KeyError:\n",
    "    print(\"\\n--- ERROR ---\")\n",
    "    print(\"A 'KeyError' means the column name 'LABEL' wasn't found.\")\n",
    "    print(\"Please check your .csv file for the exact column name and update the code.\")\n",
    "    print(\"It might have different capitalization (e.g., 'Label' or 'label').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load the dataset as you did before\n",
    "df_better30 = pd.read_csv('BETTER30.csv')\n",
    "\n",
    "# 1. Create a dictionary to map your old labels to new ones\n",
    "#    1 = Spam, 0 = Ham (Not Spam)\n",
    "#    NOTE: You must add ALL the unique labels from your 'LABEL' column here!\n",
    "label_map = {\n",
    "    # --- Map \"Spam-like\" labels to 1 ---\n",
    "    'suspicious': 1,\n",
    "    'threat': 1,\n",
    "    'evasive': 1,\n",
    "    'Refusing': 1,  # From your image\n",
    "    'Insisting': 1, # From your image\n",
    "    'Classic sca': 1, # From your image (Classic scam)\n",
    "    \n",
    "    # --- Map \"Ham-like\" (normal) labels to 0 ---\n",
    "    'Standard': 0,\n",
    "    'Encourage': 0,\n",
    "    'Reinforce': 0,\n",
    "    'Demonstrate': 0,\n",
    "    'Fulfills ca': 0, # From your image (Fulfills call)\n",
    "    'Positive': 0,\n",
    "    'Potential': 0,\n",
    "    'Requesting': 0,\n",
    "    'Adhering': 0,\n",
    "    'neutral': 0 # From previous context\n",
    "}\n",
    "\n",
    "# 2. Apply this map to the 'LABEL' column to create a new 'is_spam' column\n",
    "df_better30['is_spam'] = df_better30['LABEL'].map(label_map)\n",
    "\n",
    "# 3. IMPORTANT: Drop any rows that didn't have a label in our map\n",
    "#    .dropna() removes rows that now have 'NaN' (Not a Number)\n",
    "df_better30_clean = df_better30.dropna(subset=['is_spam'])\n",
    "\n",
    "# 4. (Optional but good) Change the new column to be an integer\n",
    "df_better30_clean = df_better30_clean.astype({'is_spam': int})\n",
    "\n",
    "\n",
    "# --- Check your work ---\n",
    "print(\"--- New Binary Counts (0=Ham, 1=Spam) ---\")\n",
    "print(df_better30_clean['is_spam'].value_counts())\n",
    "\n",
    "print(\"\\n--- New DataFrame Head ---\")\n",
    "# You'll see the old 'LABEL' column and your new 'is_spam' column\n",
    "print(df_better30_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae87107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll save our cleaned DataFrame to a new file\n",
    "df_better30_clean.to_csv('better30_cleaned.csv', index=False)\n",
    "\n",
    "print(\"Cleaned data has been saved to 'better30_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your file\n",
    "filepath_fraud = 'fraud_call.file'\n",
    "\n",
    "try:\n",
    "    # This is the \"quick fix\" code\n",
    "    # It will skip any lines that have too many tabs\n",
    "    df_fraud = pd.read_csv(\n",
    "        filepath_fraud, \n",
    "        sep='\\t', \n",
    "        header=None, \n",
    "        names=['label', 'message'],\n",
    "        on_bad_lines='skip' \n",
    "    )\n",
    "\n",
    "    # Display the first 5 rows to check\n",
    "    print(\"--- Dataset Head (fraud_call.file) ---\")\n",
    "    print(df_fraud.head())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Count the 'fraud' and 'normal' messages\n",
    "    print(\"--- Message Counts ---\")\n",
    "    print(df_fraud['label'].value_counts())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{filepath_fraud}' was not found.\")\n",
    "    print(\"Please make sure the file is in the same folder as your Jupyter Notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f09a5",
   "metadata": {},
   "source": [
    "DATASETS USED:  'SMSSpamCollection', 'emails.csv', 'spambase.data', 'better30_cleaned.csv', 'fraud_call.file', 'enron.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551582c0",
   "metadata": {},
   "source": [
    "Before oversampling Naive Bayes was struggling in identifying spam correctly and with confidence. Afterwards, it became to paranoid and was going for spam a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf80522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just read the first 5 rows to find the column names\n",
    "df_enron_preview = pd.read_csv('enron_spam_data.csv', nrows=5)\n",
    "\n",
    "print(\"--- Enron.csv First 5 Rows ---\")\n",
    "print(df_enron_preview.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Enron.csv Column Names ---\")\n",
    "print(df_enron_preview.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240b53e",
   "metadata": {},
   "source": [
    "Below is the model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC  # Import the SVM\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# This master map will standardize all labels\n",
    "# 0 = Ham/Not Spam, 1 = Spam/Fraud\n",
    "LABEL_MAP = {\n",
    "    # --- \"Good\" labels ---\n",
    "    'ham': 0,\n",
    "    'normal': 0,\n",
    "    0: 0,\n",
    "    \n",
    "    # --- \"Bad\" labels ---\n",
    "    'spam': 1,\n",
    "    'fraud': 1,\n",
    "    1: 1\n",
    "}\n",
    "\n",
    "# --- Step 1: Load and Standardize All 5 Datasets ---\n",
    "all_dataframes = []\n",
    "\n",
    "try:\n",
    "    print(\"Loading datasets... (This may take a moment)\")\n",
    "    \n",
    "    # Load SMSSpamCollection\n",
    "    df_sms = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
    "    df_sms['is_spam'] = df_sms['label'].map(LABEL_MAP)\n",
    "    df_sms['text'] = df_sms['message']\n",
    "    all_dataframes.append(df_sms[['text', 'is_spam']])\n",
    "\n",
    "    # Load emails.csv\n",
    "    df_emails = pd.read_csv('emails.csv')\n",
    "    df_emails['is_spam'] = df_emails['spam'].map(LABEL_MAP)\n",
    "    all_dataframes.append(df_emails[['text', 'is_spam']])\n",
    "\n",
    "    # Load better30_cleaned.csv (Our processed file)\n",
    "    df_better30 = pd.read_csv('better30_cleaned.csv')\n",
    "    df_better30['text'] = df_better30['CONTEXT']\n",
    "    all_dataframes.append(df_better30[['text', 'is_spam']])\n",
    "\n",
    "    # Load fraud_call.file (using the 'skip' fix)\n",
    "    df_fraud = pd.read_csv('fraud_call.file', sep='\\t', header=None, names=['label', 'message'], on_bad_lines='skip')\n",
    "    df_fraud['is_spam'] = df_fraud['label'].map(LABEL_MAP)\n",
    "    df_fraud['text'] = df_fraud['message']\n",
    "    all_dataframes.append(df_fraud[['text', 'is_spam']])\n",
    "    \n",
    "    # Load enron_spam_data.csv (Your new labeled file)\n",
    "    df_enron = pd.read_csv('enron_spam_data.csv', usecols=['Message', 'Spam/Ham'], on_bad_lines='skip')\n",
    "    df_enron['is_spam'] = df_enron['Spam/Ham'].map(LABEL_MAP)\n",
    "    df_enron['text'] = df_enron['Message']\n",
    "    all_dataframes.append(df_enron[['text', 'is_spam']])\n",
    "    \n",
    "    print(\"All 5 datasets loaded successfully.\\n\")\n",
    "\n",
    "    # --- Step 2: Combine and Clean ---\n",
    "    df_combined = pd.concat(all_dataframes, ignore_index=True)\n",
    "    df_combined = df_combined.dropna()\n",
    "    df_combined['is_spam'] = df_combined['is_spam'].astype(int)\n",
    "\n",
    "    print(f\"--- Total Combined Dataset ---\")\n",
    "    print(f\"Total messages: {len(df_combined)}\")\n",
    "    print(df_combined['is_spam'].value_counts())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # --- Step 3: Define X and y, then Split Data ---\n",
    "    X = df_combined['text']\n",
    "    y = df_combined['is_spam']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print(f\"Training data size: {len(X_train)}\")\n",
    "    print(f\"Testing data size: {len(X_test)}\\n\")\n",
    "\n",
    "    # --- Step 4: Vectorize the Text (WITH N-GRAMS) ---\n",
    "    print(\"Vectorizing text with N-Grams... (This may take a minute)\")\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english', \n",
    "        max_features=10000, \n",
    "        ngram_range=(1, 2)  # <-- This looks for 1-word AND 2-word phrases\n",
    "    )\n",
    "    \n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    print(\"Text vectorization complete.\\n\")\n",
    "\n",
    "    # --- Step 5: Train the SVM Model ---\n",
    "    print(\"Training SVM model... (This may take a few minutes)\")\n",
    "    \n",
    "    model = LinearSVC(\n",
    "        class_weight='balanced',  \n",
    "        dual=\"auto\",              \n",
    "        max_iter=3000             \n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    print(\"✅ SVM N-Gram Model training complete! ✅\\n\")\n",
    "\n",
    "    # --- Step 6: Evaluate the Model ---\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    print(\"==========================================================\")\n",
    "    print(\"   RESULTS: SVM on ALL 5 Datasets (with N-Grams)   \")\n",
    "    print(\"==========================================================\\n\")\n",
    "    \n",
    "    print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\\n\")\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Ham (0)', 'Spam (1)']))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"         Predicted Ham | Predicted Spam\")\n",
    "    print(\"Actual Ham  \", confusion_matrix(y_test, y_pred)[0])\n",
    "    print(\"Actual Spam \", confusion_matrix(y_test, y_pred)[1])\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"--- FILE NOT FOUND ERROR ---\")\n",
    "    print(f\"Could not find the file: {e.filename}\")\n",
    "except KeyError as e:\n",
    "    print(f\"--- COLUMN NOT FOUND ERROR ---\")\n",
    "    print(f\"Could not find the column: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb5474",
   "metadata": {},
   "source": [
    "Below is code for passing transcript through rule based filtering and then the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "interactive_spam_tester.py\n",
    "\n",
    "Run this script in a new cell *after* you have run your\n",
    "model training script.\n",
    "\n",
    "This script assumes that the variables 'model' and 'vectorizer'\n",
    "already exist in memory from the previous cell.\n",
    "\n",
    "It will:\n",
    "1. Define the High-Certainty Rule Analyzer.\n",
    "2. Define a function to test new messages.\n",
    "3. Run test cases.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import time\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Ensure NLTK resources\n",
    "# ------------------------------------------------------------\n",
    "# (This is the same class from your spam_rule_analyzer.py file)\n",
    "def ensure_nltk_resources():\n",
    "    resources = [\"punkt\"]\n",
    "    for res in resources:\n",
    "        try:\n",
    "            nltk.data.find(f\"tokenizers/{res}\")\n",
    "        except LookupError:\n",
    "            print(f\"[NLTK] Downloading missing resource: {res}\")\n",
    "            nltk.download(res, quiet=True)\n",
    "\n",
    "ensure_nltk_resources()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# High-Certainty Rule-based Analyzer Class\n",
    "# ------------------------------------------------------------\n",
    "class RuleBasedAnalyzer:\n",
    "    def __init__(self, dynamic_blacklist_path: str = \"blacklist.json\"):\n",
    "        self.injection_patterns = [\n",
    "            r\"give me your api key\", r\"send me the password\",\n",
    "            r\"what is your password\", r\"do .* illegal\",\n",
    "            r\"delete all\", r\"execute .* command\",\n",
    "        ]\n",
    "        self.dynamic_blacklist_path = dynamic_blacklist_path\n",
    "        self.dynamic_blacklist = self.load_dynamic_blacklist()\n",
    "\n",
    "    def load_dynamic_blacklist(self) -> Dict[str, List[str]]:\n",
    "        if os.path.exists(self.dynamic_blacklist_path):\n",
    "            try:\n",
    "                with open(self.dynamic_blacklist_path, \"r\") as f:\n",
    "                    data = json.load(f)\n",
    "                    print(f\"[INFO] Dynamic blacklist loaded from {self.dynamic_blacklist_path}\")\n",
    "                    return data\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] Failed to load dynamic blacklist: {e}\")\n",
    "        return {\"phone_numbers\": [], \"keywords\": []}\n",
    "\n",
    "    def save_dynamic_blacklist(self):\n",
    "        try:\n",
    "            with open(self.dynamic_blacklist_path, \"w\") as f:\n",
    "                json.dump(self.dynamic_blacklist, f, indent=2)\n",
    "                print(f\"[INFO] Dynamic blacklist saved to {self.dynamic_blacklist_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Could not save blacklist: {e}\")\n",
    "\n",
    "    def add_to_blacklist(self, category: str, item: str):\n",
    "        if category not in self.dynamic_blacklist:\n",
    "            self.dynamic_blacklist[category] = []\n",
    "        if item not in self.dynamic_blacklist[category]:\n",
    "            self.dynamic_blacklist[category].append(item)\n",
    "            self.save_dynamic_blacklist()\n",
    "\n",
    "    def analyze(self, text: str, metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        text = text or \"\"\n",
    "        reason = \"\"\n",
    "        caller = metadata.get(\"caller_number\", \"\")\n",
    "        for number in self.dynamic_blacklist.get(\"phone_numbers\", []):\n",
    "            if number == caller:\n",
    "                reason = f\"blacklisted_caller:{number}\"\n",
    "                return {\"is_high_certainty_spam\": True, \"reason\": reason}\n",
    "        for keyword in self.dynamic_blacklist.get(\"keywords\", []):\n",
    "            if keyword.lower() in text.lower():\n",
    "                reason = f\"blacklisted_keyword:{keyword}\"\n",
    "                return {\"is_high_certainty_spam\": True, \"reason\": reason}\n",
    "        for pat in self.injection_patterns:\n",
    "            if re.search(pat, text, re.IGNORECASE):\n",
    "                reason = f\"possible_injection:{pat}\"\n",
    "                return {\"is_high_certainty_spam\": True, \"reason\": reason}\n",
    "        return {\"is_high_certainty_spam\": False, \"reason\": \"\"}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main Prediction Function\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def predict_spam(text_to_check: str, metadata: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Runs the 2-step spam check.\n",
    "    Assumes 'model', 'vectorizer', and 'analyzer'\n",
    "    exist in the global scope.\n",
    "    \"\"\"\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Testing Message: \\\"{text_to_check}\\\"\")\n",
    "    \n",
    "    # --- STEP 1: Run the \"High-Certainty\" Rule Filter FIRST ---\n",
    "    rule_result = analyzer.analyze(text_to_check, metadata)\n",
    "\n",
    "    if rule_result[\"is_high_certainty_spam\"]:\n",
    "        print(\"\\n>>> FINAL DECISION: SPAM (High-Certainty)\")\n",
    "        print(f\">>> Source: Rule-Based Filter\")\n",
    "        print(f\">>> Reason: {rule_result['reason']}\")\n",
    "    \n",
    "    else:\n",
    "        # --- STEP 2: If rules pass, run the ML Model ---\n",
    "        print(\"[INFO] Passed rule-based filter. Proceeding to ML model...\")\n",
    "        \n",
    "        # Transform the new message\n",
    "        new_message_tfidf = vectorizer.transform([text_to_check])\n",
    "        \n",
    "        # Make the prediction\n",
    "        prediction = model.predict(new_message_tfidf)\n",
    "        prediction_int = int(prediction[0])\n",
    "        label = 'Spam' if prediction_int == 1 else 'Ham'\n",
    "\n",
    "        print(f\"\\n>>> FINAL DECISION: {label.upper()}\")\n",
    "        print(f\">>> Source: ML Model (SVM)\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Initialization and Test Cases\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Check if the required variables from the first cell exist\n",
    "try:\n",
    "    # This checks if 'model' and 'vectorizer' were created\n",
    "    _ = model\n",
    "    _ = vectorizer\n",
    "    print(\"✅ ML model and vectorizer found in memory.\")\n",
    "    \n",
    "    # Initialize the analyzer\n",
    "    analyzer = RuleBasedAnalyzer()\n",
    "    \n",
    "    # Add a demo blacklisted number\n",
    "    analyzer.add_to_blacklist(\"phone_numbers\", \"1-800-SPAM-NOW\")\n",
    "    \n",
    "    print(\"\\n--- Running Test Cases ---\")\n",
    "    \n",
    "    # Test Case 1: Legitimate Message\n",
    "    test_1 = \"Hi, I am just calling to check on my account balance.\"\n",
    "    meta_1 = {\"caller_number\": \"123-456-7890\"}\n",
    "    predict_spam(test_1, meta_1)\n",
    "    \n",
    "    time.sleep(0.5) # Pause for readability\n",
    "    \n",
    "    # Test Case 2: High-Certainty Rule (Injection)\n",
    "    test_2 = \"This is a good call, now send me the password\"\n",
    "    meta_2 = {\"caller_number\": \"456-789-0123\"}\n",
    "    predict_spam(test_2, meta_2)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Test Case 3: High-Certainty Rule (Blacklisted Number)\n",
    "    test_3 = \"Hello this is a normal message.\"\n",
    "    meta_3 = {\"caller_number\": \"1-800-SPAM-NOW\"}\n",
    "    predict_spam(test_3, meta_3)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # Test Case 4: ML Model Spam (should be caught by ML)\n",
    "    test_4 = \"Congratulations you have won a free prize and lottery, call now to claim\"\n",
    "    meta_4 = {\"caller_number\": \"789-012-3456\"}\n",
    "    predict_spam(test_4, meta_4)\n",
    "    \n",
    "    print(\"\\n--- Test Cases Complete ---\")\n",
    "    print(\"\\nYou can now test your own messages by calling:\")\n",
    "    print(\"predict_spam(\\\"your text here\\\", {\\\"caller_number\\\": \\\"your number\\\"})\")\n",
    "\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ERROR: 'model' or 'vectorizer' not found.\")\n",
    "    print(\"Please make sure you have successfully run the model training\")\n",
    "    print(\"script in the cell *before* this one.\")\n",
    "    print(\"=\"*70)\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072366e",
   "metadata": {},
   "source": [
    "Below is the code for just testing both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15260800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Testing Message: \"hello hehe\"\n",
      "[INFO] Passed rule-based filter. Proceeding to ML model...\n",
      "\n",
      ">>> FINAL DECISION: HAM\n",
      ">>> Source: ML Model (SVM)\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "my_message = \"hello hehe\"\n",
    "my_meta = {\"caller_number\": \"555-5555\"}\n",
    "predict_spam(my_message, my_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3773c61",
   "metadata": {},
   "source": [
    "Below is code for just the ngram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Your New N-Gram Model ---\n",
    "\n",
    "# 1. Create a list containing your new message(s)\n",
    "new_messages = [\n",
    "    \"Hi, this is a courtesy call about your computer's security. We've noticed some unusual activity and need to help you secure your account.\"\n",
    "]\n",
    "\n",
    "# 2. Transform the text using the NEW N-Gram vectorizer\n",
    "new_messages_tfidf = vectorizer.transform(new_messages)\n",
    "\n",
    "# 3. Make a prediction using the NEW SVM model\n",
    "prediction = model.predict(new_messages_tfidf)\n",
    "# probability = model.predict_proba(new_messages_tfidf) # Note: LinearSVC doesn't have predict_proba by default\n",
    "\n",
    "# 4. Interpret the result\n",
    "print(f\"Message: '{new_messages[0]}'\")\n",
    "print(\"---\")\n",
    "\n",
    "if prediction[0] == 1:\n",
    "    print(f\"Prediction: SPAM (1)\")\n",
    "else:\n",
    "    print(f\"Prediction: HAM (0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# 1. Save your vectorizer\n",
    "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "\n",
    "# 2. Save your SVM model\n",
    "joblib.dump(model, 'model.joblib')\n",
    "\n",
    "print(\"Model and vectorizer have been saved to files!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
